{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"cache/beit_embeddings.npz\")\n",
    "beit_embeddings = a[\"arr_0\"]\n",
    "beit_shape = beit_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_embedding = beit_embeddings\n",
    "beit_embeddings\n",
    "print(full_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_array = full_embedding[::10]\n",
    "print(\"small_array.shape = \", small_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(small_array)\n",
    "\n",
    "pca.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "#\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "#\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "#\n",
    "# Create the visualization plot\n",
    "#\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "print(cum_sum_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = scaler.transform(full_embedding)\n",
    "del full_embedding\n",
    "pca_embedding_1 = pca.transform(xx[:len(xx)//3])\n",
    "print(1)\n",
    "pca_embedding_2 = pca.transform(xx[len(xx)//3:2*(len(xx)//3)])\n",
    "print(2)\n",
    "pca_embedding_3 = pca.transform(xx[2*(len(xx)//3):])\n",
    "print(3)\n",
    "pca_embedding = np.concatenate((pca_embedding_1, pca_embedding_2, pca_embedding_3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_embedding[1795:1805, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding = np.zeros_like(pca_embedding)\n",
    "for i in range(len(pca_embedding) // 1800):\n",
    "    mean_embedding[i*1800: (i+1)*1800] = np.mean(pca_embedding[i*1800: (i+1)*1800], 0)\n",
    "print(mean_embedding[1795:1805, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pca_embedding = np.load(\"cache/pca_embedding_beit_simclr_handcrafted_weighted_by_average_motion_and_feature,simclr=0.5_handcrafted=1.8.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = np.concatenate((full_pca_embedding[:,:120], mean_embedding[:,:]), axis=1)\n",
    "print(final_submission[1795:1805, :5])\n",
    "final_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/data/behavior-representation/\"\n",
    "def validate_submission(submission):\n",
    "    frame_number_map = np.load(os.path.join(DATA_DIR, 'frame_number_map.npy'), allow_pickle=True).item()\n",
    "\n",
    "    if not isinstance(submission, np.ndarray):\n",
    "        print(\"Embeddings should be a numpy array\")\n",
    "        return False\n",
    "    elif not len(submission.shape) == 2:\n",
    "        print(\"Embeddings should be 2D array\")\n",
    "        return False\n",
    "    elif not submission.shape[1] <= 128:\n",
    "        print(\"Embeddings too large, max allowed is 128\")\n",
    "        return False\n",
    "    elif not isinstance(submission[0, 0], np.float32):\n",
    "        print(f\"Embeddings are not float32\")\n",
    "        return False\n",
    "\n",
    "    \n",
    "    total_clip_length = frame_number_map[list(frame_number_map.keys())[-1]][1]\n",
    "            \n",
    "    if not len(submission) == total_clip_length:\n",
    "        print(f\"Emebddings length doesn't match submission clips total length\")\n",
    "        return False\n",
    "\n",
    "    if not np.isfinite(submission).all():\n",
    "        print(f\"Emebddings contains NaN or infinity\")\n",
    "        return False\n",
    "\n",
    "    print(\"All checks passed\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_submission(final_submission)\n",
    "np.save(f'cache/final_submission.npy', final_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
